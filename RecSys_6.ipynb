{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44f2a402",
   "metadata": {},
   "source": [
    "Цель: научиться создавать рекомендательную систему на основе\n",
    "графовых нейронных сетей (GNN), таких как легковесная графовая нейронная\n",
    "сеть LightGCN, IRGNN (Item Relationship Graph Neural Network), применяя\n",
    "методологию анализа графа взаимодействий пользователей и товаров/услуг,\n",
    "научиться применять её для прогнозирования предпочтений пользователей,\n",
    "оценить качество полученной модели на реальных данных.\n",
    "\n",
    "1 часть – общий пример (1 балл)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5187cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch_geometric.data import Data\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "import os\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import pytorch_lightning as pl\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import dgl\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from dgl.nn.pytorch import GraphConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cab0cebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings = pd.read_csv('ml-latest-small\\\\ratings.csv')\n",
    "movies = pd.read_csv('ml-latest-small\\\\movies.csv')\n",
    "\n",
    "users = ratings['userId'].unique()\n",
    "items = movies['movieId'].unique()\n",
    "\n",
    "node_id_map = {uid: i for i, uid in enumerate(users)}\n",
    "item_id_map = {iid: len(users)+i for i, iid in enumerate(items)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "961978f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges = []\n",
    "labels = []\n",
    "for _, row in ratings.iterrows():\n",
    "    user_idx = node_id_map[row['userId']]\n",
    "    item_idx = item_id_map[row['movieId']]\n",
    "    edges.append((user_idx, item_idx))\n",
    "    labels.append(row['rating'])\n",
    "    \n",
    "edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "ratings['rating'] = scaler.fit_transform(ratings[['rating']])\n",
    "labels = torch.tensor(ratings['rating'].values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bcc51691",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = len(users) + len(items)\n",
    "x = torch.eye(num_nodes)\n",
    "\n",
    "edge_indices = list(range(edge_index.shape[1]))\n",
    "train_idx, test_idx = train_test_split(edge_indices, test_size=0.2, random_state=42)\n",
    "train_edge_index = edge_index[:, train_idx]\n",
    "train_labels = labels[train_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09085bb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecommenderModel(torch.nn.Module):\n",
    "    def __init__(self, num_features, hidden_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = GCNConv(num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, 1)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "16ccde02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.6062\n",
      "Epoch: 2, Loss: 0.2913\n",
      "Epoch: 3, Loss: 0.1618\n",
      "Epoch: 4, Loss: 0.1196\n",
      "Epoch: 5, Loss: 0.1528\n",
      "Epoch: 6, Loss: 0.1839\n",
      "Epoch: 7, Loss: 0.1756\n",
      "Epoch: 8, Loss: 0.1445\n",
      "Epoch: 9, Loss: 0.1132\n",
      "Epoch: 10, Loss: 0.0948\n",
      "Epoch: 11, Loss: 0.0908\n",
      "Epoch: 12, Loss: 0.0951\n",
      "Epoch: 13, Loss: 0.1004\n",
      "Epoch: 14, Loss: 0.1025\n",
      "Epoch: 15, Loss: 0.1001\n",
      "Epoch: 16, Loss: 0.0943\n",
      "Epoch: 17, Loss: 0.0866\n",
      "Epoch: 18, Loss: 0.0790\n",
      "Epoch: 19, Loss: 0.0732\n",
      "Epoch: 20, Loss: 0.0701\n",
      "Epoch: 21, Loss: 0.0696\n",
      "Epoch: 22, Loss: 0.0709\n",
      "Epoch: 23, Loss: 0.0726\n",
      "Epoch: 24, Loss: 0.0732\n",
      "Epoch: 25, Loss: 0.0722\n",
      "Epoch: 26, Loss: 0.0698\n",
      "Epoch: 27, Loss: 0.0667\n",
      "Epoch: 28, Loss: 0.0638\n",
      "Epoch: 29, Loss: 0.0618\n",
      "Epoch: 30, Loss: 0.0607\n",
      "Epoch: 31, Loss: 0.0604\n",
      "Epoch: 32, Loss: 0.0606\n",
      "Epoch: 33, Loss: 0.0608\n",
      "Epoch: 34, Loss: 0.0607\n",
      "Epoch: 35, Loss: 0.0602\n",
      "Epoch: 36, Loss: 0.0593\n",
      "Epoch: 37, Loss: 0.0583\n",
      "Epoch: 38, Loss: 0.0573\n",
      "Epoch: 39, Loss: 0.0565\n",
      "Epoch: 40, Loss: 0.0559\n",
      "Epoch: 41, Loss: 0.0556\n",
      "Epoch: 42, Loss: 0.0555\n",
      "Epoch: 43, Loss: 0.0555\n",
      "Epoch: 44, Loss: 0.0555\n",
      "Epoch: 45, Loss: 0.0552\n",
      "Epoch: 46, Loss: 0.0549\n",
      "Epoch: 47, Loss: 0.0544\n",
      "Epoch: 48, Loss: 0.0539\n",
      "Epoch: 49, Loss: 0.0535\n",
      "Epoch: 50, Loss: 0.0532\n",
      "Epoch: 51, Loss: 0.0530\n",
      "Epoch: 52, Loss: 0.0529\n",
      "Epoch: 53, Loss: 0.0527\n",
      "Epoch: 54, Loss: 0.0526\n",
      "Epoch: 55, Loss: 0.0525\n",
      "Epoch: 56, Loss: 0.0523\n",
      "Epoch: 57, Loss: 0.0520\n",
      "Epoch: 58, Loss: 0.0518\n",
      "Epoch: 59, Loss: 0.0516\n",
      "Epoch: 60, Loss: 0.0514\n",
      "Epoch: 61, Loss: 0.0512\n",
      "Epoch: 62, Loss: 0.0511\n",
      "Epoch: 63, Loss: 0.0510\n",
      "Epoch: 64, Loss: 0.0509\n",
      "Epoch: 65, Loss: 0.0508\n",
      "Epoch: 66, Loss: 0.0506\n",
      "Epoch: 67, Loss: 0.0505\n",
      "Epoch: 68, Loss: 0.0503\n",
      "Epoch: 69, Loss: 0.0502\n",
      "Epoch: 70, Loss: 0.0500\n",
      "Epoch: 71, Loss: 0.0499\n",
      "Epoch: 72, Loss: 0.0498\n",
      "Epoch: 73, Loss: 0.0497\n",
      "Epoch: 74, Loss: 0.0496\n",
      "Epoch: 75, Loss: 0.0495\n",
      "Epoch: 76, Loss: 0.0494\n",
      "Epoch: 77, Loss: 0.0493\n",
      "Epoch: 78, Loss: 0.0492\n",
      "Epoch: 79, Loss: 0.0491\n",
      "Epoch: 80, Loss: 0.0489\n",
      "Epoch: 81, Loss: 0.0489\n",
      "Epoch: 82, Loss: 0.0488\n",
      "Epoch: 83, Loss: 0.0487\n",
      "Epoch: 84, Loss: 0.0486\n",
      "Epoch: 85, Loss: 0.0485\n",
      "Epoch: 86, Loss: 0.0484\n",
      "Epoch: 87, Loss: 0.0483\n",
      "Epoch: 88, Loss: 0.0482\n",
      "Epoch: 89, Loss: 0.0481\n",
      "Epoch: 90, Loss: 0.0480\n",
      "Epoch: 91, Loss: 0.0479\n",
      "Epoch: 92, Loss: 0.0479\n",
      "Epoch: 93, Loss: 0.0478\n",
      "Epoch: 94, Loss: 0.0477\n",
      "Epoch: 95, Loss: 0.0476\n",
      "Epoch: 96, Loss: 0.0475\n",
      "Epoch: 97, Loss: 0.0475\n",
      "Epoch: 98, Loss: 0.0474\n",
      "Epoch: 99, Loss: 0.0473\n",
      "Epoch: 100, Loss: 0.0472\n"
     ]
    }
   ],
   "source": [
    "model = RecommenderModel(num_features=num_nodes, hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = torch.nn.MSELoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    out = model(x, train_edge_index)\n",
    "    preds = out[train_edge_index[1]]\n",
    "    loss = criterion(preds, train_labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    return float(loss)\n",
    "\n",
    "for epoch in range(1, 101):\n",
    "    loss = train()\n",
    "    print(f'Epoch: {epoch}, Loss: {loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f6bc95d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test MSE: 0.0892\n"
     ]
    }
   ],
   "source": [
    "test_edge_index = edge_index[:, test_idx]\n",
    "test_labels = labels[test_idx]\n",
    "\n",
    "def evaluate(model, x, edge_index, labels):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        pred = model(x, edge_index)\n",
    "        preds = pred[edge_index[1]]  \n",
    "        mse_loss = criterion(preds, labels)\n",
    "    return float(mse_loss)\n",
    "\n",
    "test_mse = evaluate(model, x, test_edge_index, labels[test_idx])\n",
    "print(f'Test MSE: {test_mse:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda52810",
   "metadata": {},
   "source": [
    "7. Самостоятельное задание\n",
    "\n",
    "Проведите гиперпараметризацию модели, выбрав оптимальное количество\n",
    "слоёв и размерность скрытых признаков.\n",
    "Добавьте дополнительные признаки пользователей и объектов (пол,\n",
    "возраст, жанр фильма и др.) и посмотрите влияние на точность рекомендаций."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d7d55a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden: 8, Layers: 1, Test MSE: 0.0863\n",
      "Hidden: 8, Layers: 2, Test MSE: 0.0909\n",
      "Hidden: 8, Layers: 3, Test MSE: 0.0765\n",
      "Hidden: 16, Layers: 1, Test MSE: 0.0867\n",
      "Hidden: 16, Layers: 2, Test MSE: 0.0905\n",
      "Hidden: 16, Layers: 3, Test MSE: 0.0802\n",
      "Hidden: 32, Layers: 1, Test MSE: 0.0871\n",
      "Hidden: 32, Layers: 2, Test MSE: 0.0959\n",
      "Hidden: 32, Layers: 3, Test MSE: 0.0859\n",
      "Hidden: 64, Layers: 1, Test MSE: 0.0863\n",
      "Hidden: 64, Layers: 2, Test MSE: 0.0971\n",
      "Hidden: 64, Layers: 3, Test MSE: 0.0864\n",
      "\n",
      "Best params: {'hidden_dim': 8, 'num_layers': 3}, Best Test MSE: 0.0765\n"
     ]
    }
   ],
   "source": [
    "hidden_dims = [8, 16, 32, 64]\n",
    "num_layers_list = [1, 2, 3]\n",
    "\n",
    "best_mse = float('inf')\n",
    "best_params = {}\n",
    "\n",
    "for hidden_dim in hidden_dims:\n",
    "    for num_layers in num_layers_list:\n",
    "        class FlexibleGCN(torch.nn.Module):\n",
    "            def __init__(self, num_features, hidden_dim, num_layers):\n",
    "                super().__init__()\n",
    "                self.layers = torch.nn.ModuleList()\n",
    "                if num_layers == 1:\n",
    "                    self.layers.append(GCNConv(num_features, 1))\n",
    "                else:\n",
    "                    self.layers.append(GCNConv(num_features, hidden_dim))\n",
    "                    for _ in range(num_layers - 2):\n",
    "                        self.layers.append(GCNConv(hidden_dim, hidden_dim))\n",
    "                    self.layers.append(GCNConv(hidden_dim, 1))\n",
    "            def forward(self, x, edge_index):\n",
    "                for i, conv in enumerate(self.layers):\n",
    "                    x = conv(x, edge_index)\n",
    "                    if i != len(self.layers) - 1:\n",
    "                        x = F.relu(x)\n",
    "                return x.squeeze()\n",
    "\n",
    "        model = FlexibleGCN(num_features=num_nodes, hidden_dim=hidden_dim, num_layers=num_layers)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "        for epoch in range(101): \n",
    "            model.train()\n",
    "            optimizer.zero_grad()\n",
    "            out = model(x, train_edge_index)\n",
    "            preds = out[train_edge_index[1]]\n",
    "            loss = criterion(preds, train_labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        test_mse = evaluate(model, x, test_edge_index, test_labels)\n",
    "        print(f\"Hidden: {hidden_dim}, Layers: {num_layers}, Test MSE: {test_mse:.4f}\")\n",
    "\n",
    "        if test_mse < best_mse:\n",
    "            best_mse = test_mse\n",
    "            best_params = {'hidden_dim': hidden_dim, 'num_layers': num_layers}\n",
    "\n",
    "print(f\"\\nBest params: {best_params}, Best Test MSE: {best_mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c649d24",
   "metadata": {},
   "source": [
    "3 часть – Применение графовой нейронной сети IRGNN для реализации\n",
    "рекомендательной системы (2 балла)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36769b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('ml-100k\\\\u.data', delimiter='\\t', header=None, names=[\"user_id\", \"item_id\",\n",
    "\"rating\", \"timestamp\"])\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0fc26f5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "171f8539a4bd460089cfe2a4edc5f410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1682 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "graph = nx.DiGraph()\n",
    "\n",
    "for index, group in tqdm(df.groupby(['item_id'])):\n",
    "    graph.add_node(index)\n",
    "    common_users = set(group['user_id'])\n",
    "    for other_item_id, other_group in df.groupby(['item_id']):\n",
    "        if other_item_id != index and not graph.has_edge(index, other_item_id):\n",
    "            intersection = common_users.intersection(set(other_group['user_id']))\n",
    "            weight = len(intersection)\n",
    "            if weight > 0:\n",
    "                graph.add_edge(index, other_item_id, weight=weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34e44b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dgl_graph = dgl.from_networkx(graph, edge_attrs=['weight'])\n",
    "\n",
    "class IRGNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(IRGNN, self).__init__()\n",
    "        self.conv1 = GraphConv(input_dim, hidden_dim)\n",
    "        self.conv2 = GraphConv(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, g, features):\n",
    "        h = F.relu(self.conv1(g, features))\n",
    "        h = F.relu(self.conv2(g, h))\n",
    "        return h\n",
    "\n",
    "input_dim = 16\n",
    "hidden_dim = 32\n",
    "output_dim = 8\n",
    "model = IRGNN(input_dim, hidden_dim, output_dim)\n",
    "features = torch.randn(dgl_graph.number_of_nodes(), input_dim)\n",
    "labels = torch.zeros(dgl_graph.number_of_nodes())\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "131c5a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Loss 2.0958\n",
      "Epoch 1: Loss 2.0500\n",
      "Epoch 2: Loss 2.0148\n",
      "Epoch 3: Loss 1.9821\n",
      "Epoch 4: Loss 1.9485\n",
      "Epoch 5: Loss 1.9141\n",
      "Epoch 6: Loss 1.8780\n",
      "Epoch 7: Loss 1.8399\n",
      "Epoch 8: Loss 1.7991\n",
      "Epoch 9: Loss 1.7555\n",
      "Epoch 10: Loss 1.7091\n",
      "Epoch 11: Loss 1.6600\n",
      "Epoch 12: Loss 1.6083\n",
      "Epoch 13: Loss 1.5541\n",
      "Epoch 14: Loss 1.4975\n",
      "Epoch 15: Loss 1.4386\n",
      "Epoch 16: Loss 1.3776\n",
      "Epoch 17: Loss 1.3147\n",
      "Epoch 18: Loss 1.2502\n",
      "Epoch 19: Loss 1.1843\n",
      "Epoch 20: Loss 1.1174\n",
      "Epoch 21: Loss 1.0498\n",
      "Epoch 22: Loss 0.9821\n",
      "Epoch 23: Loss 0.9145\n",
      "Epoch 24: Loss 0.8477\n",
      "Epoch 25: Loss 0.7821\n",
      "Epoch 26: Loss 0.7180\n",
      "Epoch 27: Loss 0.6558\n",
      "Epoch 28: Loss 0.5959\n",
      "Epoch 29: Loss 0.5391\n",
      "Epoch 30: Loss 0.4858\n",
      "Epoch 31: Loss 0.4363\n",
      "Epoch 32: Loss 0.3907\n",
      "Epoch 33: Loss 0.3491\n",
      "Epoch 34: Loss 0.3115\n",
      "Epoch 35: Loss 0.2778\n",
      "Epoch 36: Loss 0.2478\n",
      "Epoch 37: Loss 0.2211\n",
      "Epoch 38: Loss 0.1977\n",
      "Epoch 39: Loss 0.1770\n",
      "Epoch 40: Loss 0.1590\n",
      "Epoch 41: Loss 0.1432\n",
      "Epoch 42: Loss 0.1294\n",
      "Epoch 43: Loss 0.1173\n",
      "Epoch 44: Loss 0.1068\n",
      "Epoch 45: Loss 0.0976\n",
      "Epoch 46: Loss 0.0896\n",
      "Epoch 47: Loss 0.0825\n",
      "Epoch 48: Loss 0.0763\n",
      "Epoch 49: Loss 0.0709\n",
      "Epoch 50: Loss 0.0661\n",
      "Epoch 51: Loss 0.0618\n",
      "Epoch 52: Loss 0.0580\n",
      "Epoch 53: Loss 0.0546\n",
      "Epoch 54: Loss 0.0516\n",
      "Epoch 55: Loss 0.0489\n",
      "Epoch 56: Loss 0.0465\n",
      "Epoch 57: Loss 0.0443\n",
      "Epoch 58: Loss 0.0423\n",
      "Epoch 59: Loss 0.0405\n",
      "Epoch 60: Loss 0.0389\n",
      "Epoch 61: Loss 0.0374\n",
      "Epoch 62: Loss 0.0360\n",
      "Epoch 63: Loss 0.0348\n",
      "Epoch 64: Loss 0.0336\n",
      "Epoch 65: Loss 0.0326\n",
      "Epoch 66: Loss 0.0316\n",
      "Epoch 67: Loss 0.0307\n",
      "Epoch 68: Loss 0.0299\n",
      "Epoch 69: Loss 0.0291\n",
      "Epoch 70: Loss 0.0283\n",
      "Epoch 71: Loss 0.0277\n",
      "Epoch 72: Loss 0.0270\n",
      "Epoch 73: Loss 0.0264\n",
      "Epoch 74: Loss 0.0259\n",
      "Epoch 75: Loss 0.0253\n",
      "Epoch 76: Loss 0.0248\n",
      "Epoch 77: Loss 0.0244\n",
      "Epoch 78: Loss 0.0239\n",
      "Epoch 79: Loss 0.0235\n",
      "Epoch 80: Loss 0.0231\n",
      "Epoch 81: Loss 0.0227\n",
      "Epoch 82: Loss 0.0223\n",
      "Epoch 83: Loss 0.0219\n",
      "Epoch 84: Loss 0.0216\n",
      "Epoch 85: Loss 0.0213\n",
      "Epoch 86: Loss 0.0210\n",
      "Epoch 87: Loss 0.0207\n",
      "Epoch 88: Loss 0.0204\n",
      "Epoch 89: Loss 0.0201\n",
      "Epoch 90: Loss 0.0198\n",
      "Epoch 91: Loss 0.0196\n",
      "Epoch 92: Loss 0.0193\n",
      "Epoch 93: Loss 0.0191\n",
      "Epoch 94: Loss 0.0188\n",
      "Epoch 95: Loss 0.0186\n",
      "Epoch 96: Loss 0.0184\n",
      "Epoch 97: Loss 0.0181\n",
      "Epoch 98: Loss 0.0179\n",
      "Epoch 99: Loss 0.0177\n"
     ]
    }
   ],
   "source": [
    "labels = labels.long()\n",
    "\n",
    "for epoch in range(100):\n",
    "    logits = model(dgl_graph, features)\n",
    "    loss = F.cross_entropy(logits, labels)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    print(f\"Epoch {epoch}: Loss {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7762e989",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6816089e510c4909935f608bba1d4276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting for user 877:   0%|          | 0/1617 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[287, 301, 285, 49, 257, 293, 180, 312, 268, 299]\n"
     ]
    }
   ],
   "source": [
    "def predict_rating(user_id, item_id):\n",
    "    device = next(model.parameters()).device\n",
    "    user_rated_items = train_df[train_df.user_id == user_id]['item_id'].unique()\n",
    "    if len(user_rated_items) == 0:\n",
    "        return float('-inf')\n",
    "\n",
    "    features_dev = features.to(device)\n",
    "    g_dev = dgl_graph.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_item_emb = model(g_dev, features_dev)\n",
    "\n",
    "        if item_id not in g_dev.ndata[dgl.NID].tolist() if 'nid' in g_dev.ndata else False:\n",
    "            return float('-inf')\n",
    "\n",
    "        target_emb = all_item_emb[item_id]\n",
    "\n",
    "        rated_embs = []\n",
    "        for iid in user_rated_items:\n",
    "            if iid < all_item_emb.size(0):\n",
    "                rated_embs.append(all_item_emb[iid])\n",
    "        if len(rated_embs) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        user_emb = torch.stack(rated_embs, dim=0).mean(dim=0)\n",
    "\n",
    "        score = torch.dot(user_emb, target_emb).item()\n",
    "\n",
    "    return score\n",
    "\n",
    "\n",
    "def recommend_items(user_id, top_k=10):\n",
    "    items_rated = set(train_df[train_df.user_id == user_id]['item_id'].tolist())\n",
    "\n",
    "    all_items = set(df['item_id'].unique())\n",
    "    unrated_items = list(all_items - items_rated)\n",
    "    if len(items_rated) == 0:\n",
    "        return []\n",
    "    predictions = {}\n",
    "\n",
    "    for iid in tqdm(unrated_items, desc=f\"Predicting for user {user_id}\"):\n",
    "        try:\n",
    "            score = predict_rating(user_id, iid)\n",
    "        except Exception:\n",
    "            continue\n",
    "        predictions[iid] = score\n",
    "\n",
    "    if not predictions:\n",
    "        return []\n",
    "\n",
    "    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_items = [item_id for item_id, _ in sorted_predictions[:top_k]]\n",
    "    return top_items\n",
    "    \n",
    "recommendations = recommend_items(test_df.iloc[0].user_id, top_k=10)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443061f5",
   "metadata": {},
   "source": [
    "Самостоятельное задание\\\n",
    "Примените предложенную реализацию на другом открытом наборе данных\n",
    "lastfm.zip.\n",
    "Попробуйте реализовать расширенные версии модели (например, добавив\n",
    "веса ребрам графа или учитывая временные факторы при формировании связей)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "19ebe1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_path = os.path.join(\"lastfm\", \"lastfm.edges\")\n",
    "colnames = [\"user_id\", \"item_id\", \"rating_dummy\", \"timestamp\"]\n",
    "lastfm_edges = pd.read_csv(edges_path, sep=\",\", names=colnames, header=None)\n",
    "\n",
    "types_path = os.path.join(\"lastfm\", \"lastfm.types\")\n",
    "try:\n",
    "    lastfm_types = pd.read_csv(types_path, sep=\",\", names=[\"item_id\", \"type\"], header=None)\n",
    "except FileNotFoundError:\n",
    "    lastfm_types = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "aff63bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_lastfm_edges = lastfm_edges.sample(n=5000, random_state=42).reset_index(drop=True)\n",
    "\n",
    "train_lastfm, test_lastfm = train_test_split(\n",
    "    small_lastfm_edges, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "train_df = train_lastfm.copy()\n",
    "test_df  = test_lastfm.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "bef5ac6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2332ef3302794a269386ec2f9bbe4525",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Building item-item edges:   0%|          | 0/3385 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "grouped = train_df.groupby(\"item_id\")[[\"user_id\", \"timestamp\"]]\n",
    "\n",
    "item2df = { item: group_df.reset_index(drop=True) \n",
    "            for item, group_df in grouped }\n",
    "\n",
    "all_items = list(item2df.keys())\n",
    "graph = nx.DiGraph()\n",
    "\n",
    "for item in all_items:\n",
    "    graph.add_node(item)\n",
    "\n",
    "item2user2time = {}\n",
    "for item, df_item in item2df.items():\n",
    "    item2user2time[item] = dict(\n",
    "        zip(df_item[\"user_id\"].values, df_item[\"timestamp\"].values)\n",
    "    )\n",
    "\n",
    "\n",
    "n_items = len(all_items)\n",
    "for idx_i in tqdm(range(n_items), desc=\"Building item-item edges\"):\n",
    "    item_i = all_items[idx_i]\n",
    "    user_times_i = item2user2time[item_i]\n",
    "    users_i = set(user_times_i.keys())\n",
    "\n",
    "    for idx_j in range(idx_i + 1, n_items):\n",
    "        item_j = all_items[idx_j]\n",
    "        user_times_j = item2user2time[item_j]\n",
    "        users_j = set(user_times_j.keys())\n",
    "\n",
    "        common_users = users_i.intersection(users_j)\n",
    "        if not common_users:\n",
    "            continue\n",
    "\n",
    "        deltas = []\n",
    "        for u in common_users:\n",
    "            t_i = user_times_i[u]\n",
    "            t_j = user_times_j[u]\n",
    "            deltas.append(abs(t_i - t_j) / 86400.0)\n",
    "        avg_delta_days = np.mean(deltas) if deltas else 0.0\n",
    "\n",
    "        weight_ij = len(common_users) / (1.0 + avg_delta_days)\n",
    "        weight_ji = weight_ij \n",
    "\n",
    "        graph.add_edge(item_i, item_j, weight=weight_ij)\n",
    "        graph.add_edge(item_j, item_i, weight=weight_ji)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "0ab5b20b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010, Loss: 0.2358\n",
      "Epoch 020, Loss: 0.0475\n",
      "Epoch 030, Loss: 0.0061\n",
      "Epoch 040, Loss: 0.0020\n",
      "Epoch 050, Loss: 0.0012\n",
      "Epoch 060, Loss: 0.0010\n",
      "Epoch 070, Loss: 0.0008\n",
      "Epoch 080, Loss: 0.0008\n",
      "Epoch 090, Loss: 0.0007\n",
      "Epoch 100, Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "dgl_graph = dgl.from_networkx(graph, edge_attrs=['weight'])\n",
    "dgl_graph = dgl.add_self_loop(dgl_graph)\n",
    "\n",
    "num_nodes = dgl_graph.number_of_nodes() \n",
    "input_dim = features.size(1)           \n",
    "features = torch.randn(num_nodes, input_dim)\n",
    "\n",
    "labels = torch.zeros(num_nodes).long()\n",
    "\n",
    "device = torch.device('cpu')\n",
    "model.to(device)\n",
    "features = features.to(device)\n",
    "dgl_graph = dgl_graph.to(device)\n",
    "\n",
    "for epoch in range(100):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    logits = model(dgl_graph, features) \n",
    "    loss = F.cross_entropy(logits, labels.to(device))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch {epoch+1:03d}, Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d14717c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_items_sorted = sorted(all_items) \n",
    "item_to_idx = { item: i for i, item in enumerate(all_items_sorted) }\n",
    "idx_to_item = { i: item for item, i in item_to_idx.items() }\n",
    "\n",
    "def predict_rating(user_id, item_id):\n",
    "    device = next(model.parameters()).device\n",
    "    user_rated_items = train_df[train_df.user_id == user_id]['item_id'].unique()\n",
    "    if len(user_rated_items) == 0:\n",
    "        return float('-inf')\n",
    "\n",
    "    if item_id not in item_to_idx:\n",
    "        return float('-inf')\n",
    "\n",
    "    g_dev = dgl_graph.to(device)\n",
    "    features_dev = features.to(device)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        global all_item_emb\n",
    "        all_item_emb = model(g_dev, features_dev) \n",
    "\n",
    "        node_idx = item_to_idx[item_id]\n",
    "        target_emb = all_item_emb[node_idx]  \n",
    "\n",
    "        rated_embs = []\n",
    "        for iid in user_rated_items:\n",
    "            if iid not in item_to_idx:\n",
    "                continue\n",
    "            idx = item_to_idx[iid]\n",
    "            rated_embs.append(all_item_emb[idx])\n",
    "        if len(rated_embs) == 0:\n",
    "            return float('-inf')\n",
    "\n",
    "        user_emb = torch.stack(rated_embs, dim=0).mean(dim=0)\n",
    "        score = torch.dot(user_emb, target_emb).item()\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775a4b5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3733b63a60a247a591e6cefa4641fe99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Predicting for user 459:   0%|          | 0/1258695 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Рекомендации для LastFM-пользователя 459: [48089, 1093107, 796668, 68119, 1094994, 1109971, 1210870, 1085692, 1100689, 1174619]\n"
     ]
    }
   ],
   "source": [
    "def recommend_items(user_id, top_k=10):\n",
    "    items_rated = set(train_df[train_df.user_id == user_id]['item_id'].tolist())\n",
    "    all_items_set = set(lastfm_edges['item_id'].unique())\n",
    "    unrated_items = list(all_items_set - items_rated)\n",
    "    if len(items_rated) == 0:\n",
    "        return []\n",
    "\n",
    "    device = next(model.parameters()).device\n",
    "    g_dev = dgl_graph\n",
    "    features_dev = features\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        all_item_emb = model(g_dev, features_dev) \n",
    "\n",
    "    rated_embs = []\n",
    "    for iid in items_rated:\n",
    "        if iid not in item_to_idx:\n",
    "            continue\n",
    "        idx = item_to_idx[iid]\n",
    "        rated_embs.append(all_item_emb[idx])\n",
    "    if len(rated_embs) == 0:\n",
    "        return []\n",
    "\n",
    "    user_emb = torch.stack(rated_embs, dim=0).mean(dim=0) \n",
    "\n",
    "    predictions = {}\n",
    "    for iid in tqdm(unrated_items, desc=f\"Predicting for user {user_id}\"):\n",
    "        if iid not in item_to_idx:\n",
    "            continue\n",
    "        idx = item_to_idx[iid]\n",
    "        target_emb = all_item_emb[idx]\n",
    "        score = torch.dot(user_emb, target_emb).item()\n",
    "        predictions[iid] = score\n",
    "\n",
    "    if not predictions:\n",
    "        return []\n",
    "\n",
    "    sorted_predictions = sorted(predictions.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_items = [item_id for item_id, _ in sorted_predictions[:top_k]]\n",
    "    return top_items\n",
    "\n",
    "sample_user = test_df.iloc[0].user_id\n",
    "recs = recommend_items(sample_user, top_k=10)\n",
    "print(f\"Рекомендации для LastFM-пользователя {sample_user}: {recs}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06fad0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
